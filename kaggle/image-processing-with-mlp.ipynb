{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0447d6",
   "metadata": {
    "papermill": {
     "duration": 0.004216,
     "end_time": "2024-01-08T10:12:19.175721",
     "exception": false,
     "start_time": "2024-01-08T10:12:19.171505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![image.png](https://cdn-coiao.nitrocdn.com/CYHudqJZsSxQpAPzLkHFOkuzFKDpEHGF/assets/images/optimized/rev-7841bc2/www.sharpsightlabs.com/wp-content/uploads/2020/03/numpy-flatten-visua-example.png)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A flattened image refers to the transformation of a two-dimensional array, such as an image represented in a matrix form (like a grid of pixel values), into a one-dimensional array. For example, in the context of image data, a grayscale image of dimensions 28x28 pixels can be reshaped or \"flattened\" into a single row of 784 (28x28) pixel values. This transformation is essential when using a Multilayer Perceptron (MLP) model, which typically requires inputs to be in a flat format, commonly represented as a single vector or array.\n",
    "\n",
    "If you're interested in exploring similar data analysis or learning more about AI applications, then checkout my personal website https://hughiephan.co . Don't forget to upvote if you found the notebook insightful or helpful. Your feedback is valuable and can help others discover useful content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d788e0c",
   "metadata": {
    "papermill": {
     "duration": 0.003374,
     "end_time": "2024-01-08T10:12:19.183590",
     "exception": false,
     "start_time": "2024-01-08T10:12:19.180216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f298c3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-08T10:12:19.192584Z",
     "iopub.status.busy": "2024-01-08T10:12:19.192161Z",
     "iopub.status.idle": "2024-01-08T10:12:33.296062Z",
     "shell.execute_reply": "2024-01-08T10:12:33.295102Z"
    },
    "papermill": {
     "duration": 14.111548,
     "end_time": "2024-01-08T10:12:33.298664",
     "exception": false,
     "start_time": "2024-01-08T10:12:19.187116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d543f",
   "metadata": {
    "papermill": {
     "duration": 0.003287,
     "end_time": "2024-01-08T10:12:33.305890",
     "exception": false,
     "start_time": "2024-01-08T10:12:33.302603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import dataset\n",
    "\n",
    "Fashion MNIST is a dataset containing grayscale images of clothing items (like shirts, shoes, dresses, etc.) in 28x28 pixel format, commonly used for learning computer vision tasks. Then we separate it into training and testing subsets, each with their respective images `X` and labels `Y`, which can then be used for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc0702d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T10:12:33.314779Z",
     "iopub.status.busy": "2024-01-08T10:12:33.314097Z",
     "iopub.status.idle": "2024-01-08T10:12:37.338746Z",
     "shell.execute_reply": "2024-01-08T10:12:37.337553Z"
    },
    "papermill": {
     "duration": 4.032165,
     "end_time": "2024-01-08T10:12:37.341538",
     "exception": false,
     "start_time": "2024-01-08T10:12:33.309373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e352a0",
   "metadata": {
    "papermill": {
     "duration": 0.006695,
     "end_time": "2024-01-08T10:12:37.355499",
     "exception": false,
     "start_time": "2024-01-08T10:12:37.348804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Normalization\n",
    "\n",
    "Scale the pixel intensities down to the 0-1 range by dividing them by 255.0 . We also the limit the number of images to 5000 by using `[:5000]` for a faster training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce74a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T10:12:37.371232Z",
     "iopub.status.busy": "2024-01-08T10:12:37.370856Z",
     "iopub.status.idle": "2024-01-08T10:12:37.596228Z",
     "shell.execute_reply": "2024-01-08T10:12:37.595254Z"
    },
    "papermill": {
     "duration": 0.236307,
     "end_time": "2024-01-08T10:12:37.598801",
     "exception": false,
     "start_time": "2024-01-08T10:12:37.362494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255. \n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6446e9f3",
   "metadata": {
    "papermill": {
     "duration": 0.006767,
     "end_time": "2024-01-08T10:12:37.612894",
     "exception": false,
     "start_time": "2024-01-08T10:12:37.606127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define model\n",
    "\n",
    "Our model architecture is defined sequentially with layers: first, a `Flatten` layer reshapes the `28x28` input images into a flat 1D array. Subsequently, two Dense layers follow, each with ReLU activation functionsâ€”`300` neurons in the first layer and `100` neurons in the second. The final Dense layer, consisting of `10` neurons with a softmax activation function, produces probabilities for the `10` classes present in the Fashion MNIST dataset. Our training settings will use `sparse categorical cross-entropy` loss which is suitable for multi-class classification, and Stochastic Gradient Descent `SGD` optimizer, and tracking accuracy as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61eda3fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T10:12:37.628798Z",
     "iopub.status.busy": "2024-01-08T10:12:37.628424Z",
     "iopub.status.idle": "2024-01-08T10:12:37.860069Z",
     "shell.execute_reply": "2024-01-08T10:12:37.857708Z"
    },
    "papermill": {
     "duration": 0.247497,
     "end_time": "2024-01-08T10:12:37.867461",
     "exception": false,
     "start_time": "2024-01-08T10:12:37.619964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fb48f",
   "metadata": {
    "papermill": {
     "duration": 0.00784,
     "end_time": "2024-01-08T10:12:37.883347",
     "exception": false,
     "start_time": "2024-01-08T10:12:37.875507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training\n",
    "\n",
    "We start by training the defined model using the `fit` method on the training dataset (X_train and y_train) over 5 epochs. During this process, the model learns from the input features `X_train` and their corresponding target labels `y_train`. Additionally, it validates the model's performance on a separate validation dataset `X_valid` and `y_valid` after each epoch, allowing observation of how well the model generalizes to new, unseen data. Following the training process, the evaluate method is used to assess the model's performance using a different set of unseen data, the test dataset `X_test` and `y_test`. This step provides insights into the model's accuracy and other relevant metrics when confronted with data it hasn't encountered during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924eaf7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T10:12:37.902940Z",
     "iopub.status.busy": "2024-01-08T10:12:37.902548Z",
     "iopub.status.idle": "2024-01-08T10:13:07.202826Z",
     "shell.execute_reply": "2024-01-08T10:13:07.201651Z"
    },
    "papermill": {
     "duration": 29.312699,
     "end_time": "2024-01-08T10:13:07.205507",
     "exception": false,
     "start_time": "2024-01-08T10:12:37.892808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.7187 - accuracy: 0.7615 - val_loss: 0.5087 - val_accuracy: 0.8256\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4876 - accuracy: 0.8295 - val_loss: 0.4415 - val_accuracy: 0.8502\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4422 - accuracy: 0.8442 - val_loss: 0.4191 - val_accuracy: 0.8510\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4155 - accuracy: 0.8540 - val_loss: 0.4079 - val_accuracy: 0.8606\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3944 - accuracy: 0.8624 - val_loss: 0.3882 - val_accuracy: 0.8676\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4246 - accuracy: 0.8501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4246431291103363, 0.8500999808311462]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, y_valid))\n",
    "model.evaluate(X_test, y_test) # Output test_loss and test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2629f",
   "metadata": {
    "papermill": {
     "duration": 0.045981,
     "end_time": "2024-01-08T10:13:07.293442",
     "exception": false,
     "start_time": "2024-01-08T10:13:07.247461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualize the Flatten Layer\n",
    "\n",
    "Seelct the first image from the test dataset, then reshape and normalize to scale its pixel values between 0 and 1. After that, we retrieve the specific layer `flatten` from our previously defined Neural Network, therefore creating a new model that captures the output specifically from this `flatten` layer only. Using this specialized model, it will output the `flatten` layer given the original image. The way of flatten an image like this allows for the use of MLP Neural Network model.\n",
    "\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), <------ We are looking at this layer\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971fe4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T10:13:07.384325Z",
     "iopub.status.busy": "2024-01-08T10:13:07.383925Z",
     "iopub.status.idle": "2024-01-08T10:13:08.267237Z",
     "shell.execute_reply": "2024-01-08T10:13:08.265930Z"
    },
    "papermill": {
     "duration": 0.929528,
     "end_time": "2024-01-08T10:13:08.270209",
     "exception": false,
     "start_time": "2024-01-08T10:13:07.340681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n",
      "Original Image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After flatten\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAHlkAAAAdCAYAAAA9Qi8YAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAIJ0lEQVR4nO3dOctVVx/G4eMUh0jUOA8YBYc0IoggaitYxSKVlmqrIhiwE2wUWxu/h42FnR9AJBCwEFScxTlKNCY++QAW5/fynmSLz3XVN9vt2Wv4r7W3rhlTU1NTIwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgP/YzKFvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmJ4csgwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzCIcsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMAgHLIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMwiHLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAIByyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADMIhywAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwCAcsgwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAxidg3OmDHj37wPAAAAAAAAAIAv0qxZs1Lu06dPKTc1NfX/3M5n5s6dm3IfPnxIuU2bNo3N3Lp1K10LqvptUs2V/rh27dp0rd27d6fclStXUu7du3cp9zU4ffp0yl24cOFfvhOmmzJWTHo+PnjwYMrt2rUr5W7fvp1yFy9eTLkhrFixIuX279+fcgsXLky5S5cupRwUs2e3T63/+uuvlNu5c2fK/fTTTyn3xx9/pFyte9atW5dyL168GJuZP39+utbdu3dTbunSpSn33Xffpdz9+/dT7pdffkk5preZM2emXN03+Pbbb1PuzJkzKVf3A+r9bdiwYWxm8eLF6VovX75MuXnz5qVc/e3u3buXcvXZ1t/45MmTKcfn6n5AqfPrc61rhpqre4x///13ylWnTp1KuToGvH//fmymjBOj0Wj0ww8/pNzr169Trv7GtU6pbeXx48cpV+uU0t6PHz+ervW17ENNcgz40g01Vhw6dCjlduzYMTZz/fr1dK05c+ak3N69e1OursvKemY0Go2WLVuWct9//33K1Wdb6pRaay1YsCDlzp8/n3I3btxIuemkzhU1V/v2UOPd4cOHx2a2bt2arvXw4cOUq3/XOlfUvZkHDx6kXK0r6phX+m1dHz158iTlao1S11t1Hhjq3VDtj8Wk22etyYeyZs2asZmff/45Xav2nTpv1/cH33zzTcrVvchaLzx9+nSi1ytqW6974M+fP0+5+hvXNlDfNR04cCDlSk026Vr7S7d8+fKUq2v4H3/8cWxm9erV6Vp//vlnypW9itGojxV1bqzze23vZa6t83G9t1evXqVc7Rd13VP3Kz5+/Jhy9dkuWrRobObcuXPpWr/99lvKVStXrky52hdrW6m52o43btw4NlPXqvX5v337NuXq3Djpvl1rvDL21HGx9sVVq1al3LNnz1Ku9LHRqNcL9R1SbcdLliwZm6njU32vfvbs2ZS7du1ayu3bty/lSu0+GvV+Vmuy0s9qn5j0nkudK+qau67z6vfdpb3XP7P27UnP23VtUddHmzdvTrmqrC/rOFvHu0nOn6NR/x63tvejR4+m3M2bN1Pu0aNHYzP1vXptn5OeG6taL9Q5r9bbpV6oY0Vtn3Wvoo53dd6u9XFVntmk20kdP2t9XNe+dX4/duxYytVa8MSJE2Mz27ZtS9cqddv/or5D/P3331Ou7lnWXP0eoowpdQyo+5/1nU9tx3XPpe69b9++PeV+/fXXsZk7d+6ka9WavI6Lk37nV9t7fQ/25s2blDty5MjYzFDvmmttVP+Nx5YtW1Juz549KVfXW6X/1Hl20t/g1HVeHRdr/Xn16tWUq//Wr861k3T58uWUW79+fcrVebvOeTVXxp66Fqjf7X8t38QBAAAAAABMZ+n/BvkP7gMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOAzDlkGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG4ZBlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgEA5ZBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABuGQZQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYBAOWQYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAbhkGUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGAQDlkGAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG4ZBlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgEDOmpqamhr4JAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABg+pk59A0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADTk0OWAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAQThkGQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGIRDlgEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgEE4ZBkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABiEQ5YBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBBOGQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYhEOWAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAQThkGQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGMQ/7Kp2QhiOvCcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 10000x5000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = X_test[0]  \n",
    "input_data = image.reshape(1, 28, 28) / 255.0\n",
    "flatten_layer = model.get_layer('flatten')\n",
    "visualization_model = keras.models.Model(model.input, flatten_layer.output)\n",
    "output_data = visualization_model.predict(input_data) \n",
    "print(\"Original Image\")\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"After flatten\")\n",
    "plt.figure(figsize=(100, 50)) \n",
    "plt.imshow(output_data, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546c69d",
   "metadata": {
    "papermill": {
     "duration": 0.045098,
     "end_time": "2024-01-08T10:13:08.385190",
     "exception": false,
     "start_time": "2024-01-08T10:13:08.340092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reference\n",
    "- https://www.sharpsightlabs.com/blog/numpy-flatten"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 53.897461,
   "end_time": "2024-01-08T10:13:09.954094",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-08T10:12:16.056633",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
